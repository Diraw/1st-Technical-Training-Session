{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNN(\n",
      "  (fc1): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (fc2): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "神经元（Neuron）\n",
    "神经元是神经网络的基本单元，它接收输入信号，通过加权求和后与偏置（bias）相加，然后通过激活函数处理以产生输出。\n",
    "\n",
    "神经元的权重和偏置是网络学习过程中需要调整的参数\n",
    "\n",
    "在 PyTorch 中，构建神经网络通常需要继承 nn.Module 类。\n",
    "\n",
    "nn.Module 是所有神经网络模块的基类，你需要定义以下两个部分：\n",
    "\n",
    "__init__()：定义网络层。\n",
    "forward()：定义数据的前向传播过程。\n",
    "简单的全连接神经网络（Fully Connected Network）\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义一个简单的神经网络模型\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # 定义一个输入层到隐藏层的全连接层\n",
    "        self.fc1 = nn.Linear(2, 2)  # 输入 2 个特征，输出 2 个特征\n",
    "        # 定义一个隐藏层到输出层的全连接层\n",
    "        self.fc2 = nn.Linear(2, 1)  # 输入 2 个特征，输出 1 个预测值\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 前向传播过程\n",
    "        x = torch.relu(self.fc1(x))  # 使用 ReLU 激活函数\n",
    "        x = self.fc2(x)  # 输出层\n",
    "        return x\n",
    "\n",
    "# 创建模型实例\n",
    "model = SimpleNN()\n",
    "\n",
    "# 打印模型\n",
    "print(model)\n",
    "\n",
    "''''\n",
    "'PyTorch 提供了许多常见的神经网络层，以下是几个常见的：\n",
    "\n",
    "nn.Linear(in_features, out_features)：全连接层，输入 in_features 个特征，输出 out_features 个特征。\n",
    "nn.Conv2d(in_channels, out_channels, kernel_size)：2D 卷积层，用于图像处理。\n",
    "nn.MaxPool2d(kernel_size)：2D 最大池化层，用于降维。\n",
    "nn.ReLU()：ReLU 激活函数，常用于隐藏层。\n",
    "nn.Softmax(dim)：Softmax 激活函数，通常用于输出层，适用于多类分类问题。'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "激活函数（Activation Function）\n",
    "激活函数决定了神经元是否应该被激活。它们是非线性函数，(如果没有激活函数，神经网络无论多少层都只能表示线性变换（即多个矩阵乘法的叠加仍然是线性运算）。\n",
    "激活函数对输入信号进行变换，输出一个值（称为“激活值”），决定该神经元是否传递信息到下一层。\n",
    "使得神经网络能够学习和执行更复杂的任务。常见的激活函数包括：\n",
    "\n",
    "Sigmoid：用于二分类问题，输出值在 0 和 1 之间。\n",
    "Tanh：输出值在 -1 和 1 之间，常用于输出层之前。\n",
    "ReLU（Rectified Linear Unit）：目前最流行的激活函数之一，定义为 f(x) = max(0, x)，有助于解决梯度消失问题。\n",
    "Softmax：常用于多分类问题的输出层，将输出转换为概率分布。\n",
    "'''\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ReLU 激活\n",
    "output = F.relu(input_tensor)\n",
    "\n",
    "# Sigmoid 激活\n",
    "output = torch.sigmoid(input_tensor)\n",
    "\n",
    "# Tanh 激活\n",
    "output = torch.tanh(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "损失函数（Loss Function）\n",
    "损失函数用于衡量模型的预测值与真实值之间的差异。\n",
    "\n",
    "常见的损失函数包括：\n",
    "\n",
    "均方误差（MSELoss）：回归问题常用，计算输出与目标值的平方差。\n",
    "交叉熵损失（CrossEntropyLoss）：分类问题常用，计算输出和真实标签之间的交叉熵。\n",
    "BCEWithLogitsLoss：二分类问题，结合了 Sigmoid 激活和二元交叉熵损失。\n",
    "'''\n",
    "# 均方误差损失\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 交叉熵损失\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 二分类交叉熵损失\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "优化器（Optimizer）\n",
    "优化器负责在训练过程中更新网络的权重和偏置。\n",
    "\n",
    "常见的优化器包括：\n",
    "\n",
    "SGD（随机梯度下降）\n",
    "Adam（自适应矩估计）\n",
    "RMSprop（均方根传播）\n",
    "'''\n",
    "import torch.optim as optim\n",
    "\n",
    "# 使用 SGD 优化器\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# 使用 Adam 优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "训练过程（Training Process）\n",
    "训练神经网络涉及以下步骤：\n",
    "\n",
    "准备数据：通过 DataLoader 加载数据。\n",
    "定义损失函数和优化器。\n",
    "前向传播：计算模型的输出。\n",
    "计算损失：与目标进行比较，得到损失值。\n",
    "反向传播：通过 loss.backward() 计算梯度。\n",
    "更新参数：通过 optimizer.step() 更新模型的参数。\n",
    "重复上述步骤，直到达到预定的训练轮数。\n",
    "'''\n",
    "# 假设已经定义好了模型、损失函数和优化器\n",
    "\n",
    "# 训练数据示例\n",
    "X = torch.randn(10, 2)  # 10 个样本，每个样本有 2 个特征\n",
    "Y = torch.randn(10, 1)  # 10 个目标标签\n",
    "\n",
    "# 训练过程\n",
    "for epoch in range(100):  # 训练 100 轮\n",
    "    model.train()  # 设置模型为训练模式\n",
    "    optimizer.zero_grad()  # 清除梯度\n",
    "    output = model(X)  # 前向传播\n",
    "    loss = criterion(output, Y)  # 计算损失\n",
    "    loss.backward()  # 反向传播\n",
    "    optimizer.step()  # 更新权重\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:  # 每 10 轮输出一次损失\n",
    "        print(f'Epoch [{epoch + 1}/100], Loss: {loss.item():.4f}')\n",
    "\n",
    "''' \n",
    "测试与评估\n",
    "训练完成后，需要对模型进行测试和评估。\n",
    "\n",
    "常见的步骤包括：\n",
    "\n",
    "计算测试集的损失：测试模型在未见过的数据上的表现。\n",
    "计算准确率（Accuracy）：对于分类问题，计算正确预测的比例。\n",
    "'''\n",
    "# 假设你有测试集 X_test 和 Y_test\n",
    "model.eval()  # 设置模型为评估模式\n",
    "with torch.no_grad():  # 在评估过程中禁用梯度计算\n",
    "    output = model(X_test)\n",
    "    loss = criterion(output, Y_test)\n",
    "    print(f'Test Loss: {loss.item():.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "神经网络类型\n",
    "前馈神经网络（Feedforward Neural Networks）：数据单向流动，从输入层到输出层，无反馈连接。\n",
    "卷积神经网络（Convolutional Neural Networks, CNNs）：适用于图像处理，使用卷积层提取空间特征。\n",
    "循环神经网络（Recurrent Neural Networks, RNNs）：适用于序列数据，如时间序列分析和自然语言处理，允许信息反馈循环。\n",
    "长短期记忆网络（Long Short-Term Memory, LSTM）：一种特殊的RNN，能够学习长期依赖关系。\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
