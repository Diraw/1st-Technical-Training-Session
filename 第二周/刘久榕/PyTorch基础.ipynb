{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[-1.7078,  0.4638, -2.0887],\n",
      "        [ 0.1624,  1.4070, -0.3669]])\n",
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n",
      "tensor([[-0.0375, -0.6789, -0.3075],\n",
      "        [-0.7600, -0.3400, -0.1609]])\n"
     ]
    }
   ],
   "source": [
    "''''\n",
    "'张量（Tensor）是 PyTorch 中的核心数据结构，用于存储和操作多维数组。\n",
    "\n",
    "张量可以视为一个多维数组，支持加速计算的操作。\n",
    "\n",
    "在 PyTorch 中，张量的概念类似于 NumPy 中的数组，但是 PyTorch 的张量可以运行在不同的设备上，比如 CPU 和 GPU，这使得它们非常适合于进行大规模并行计算，特别是在深度学习领域。\n",
    "\n",
    "维度（Dimensionality）：张量的维度指的是数据的多维数组结构。例如，一个标量（0维张量）是一个单独的数字，一个向量（1维张量）是一个一维数组，一个矩阵（2维张量）是一个二维数组，以此类推。\n",
    "(衡量性质的量的多少，如七天的温度，维度只有温度一维，多个人的成绩，既要展示人，也要展示成绩，是二维的，一周班级每个人每天都成绩，显然是三维的)\n",
    "\n",
    "形状（Shape）：张量的形状是指每个维度上的大小。例如，一个形状为(3, 4)的张量意味着它有3行4列。\n",
    "\n",
    "数据类型（Dtype）：张量中的数据类型定义了存储每个元素所需的内存大小和解释方式。PyTorch支持多种数据类型，包括整数型（如torch.int8、torch.int32）、浮点型（如torch.float32、torch.float64）和布尔型（torch.bool）。'\n",
    "'''\n",
    "\n",
    "import torch\n",
    "\n",
    "# 创建一个 2x3 的全 0 张量\n",
    "a = torch.zeros(2, 3)\n",
    "print(a)\n",
    "\n",
    "# 创建一个 2x3 的全 1 张量\n",
    "b = torch.ones(2, 3)\n",
    "print(b)\n",
    "\n",
    "# 创建一个 2x3 的随机数张量\n",
    "c = torch.randn(2, 3)\n",
    "print(c)\n",
    "\n",
    "# 从 NumPy 数组创建张量\n",
    "import numpy as np\n",
    "numpy_array = np.array([[1, 2], [3, 4]])\n",
    "tensor_from_numpy = torch.from_numpy(numpy_array)\n",
    "print(tensor_from_numpy)\n",
    "\n",
    "# 在指定设备（CPU/GPU）上创建张量\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "d = torch.randn(2, 3, device=device)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8461,  1.0579, -0.4861],\n",
      "        [-1.2303, -0.6165,  0.6683]])\n",
      "tensor([[ 1.0630, -0.5015,  0.2411],\n",
      "        [-1.9848, -0.4840, -0.3619]])\n",
      "tensor([[ 1.9090,  0.5564, -0.2450],\n",
      "        [-3.2151, -1.1005,  0.3065]])\n",
      "tensor([[ 0.8993, -0.5305, -0.1172],\n",
      "        [ 2.4420,  0.2984, -0.2418]])\n",
      "tensor([[-0.3368,  2.3462,  1.4429],\n",
      "        [ 0.5862,  0.0476, -0.1127]])\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "#常用张量操作：\n",
    "# 张量相加\n",
    "import torch\n",
    "e = torch.randn(2, 3)\n",
    "f = torch.randn(2, 3)\n",
    "print(e)\n",
    "print(f)\n",
    "print(e + f)\n",
    "\n",
    "# 逐元素乘法\n",
    "print(e * f)\n",
    "\n",
    "# 张量的转置,将张量的行和列交换（即转置）。\n",
    "g = torch.randn(3, 2)\n",
    "print(g.t())  # 或者 g.transpose(0, 1)\n",
    "\n",
    "# 张量的形状\n",
    "print(g.shape)  # 返回形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], requires_grad=True)\n",
      "tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "#PyTorch的张量支持自动微分，这是深度学习中的关键特性。当你创建一个需要张量的梯度时，PyTorch可以自动计算其梯度：\n",
    "import torch\n",
    "# 创建一个需要梯度的张量\n",
    "tensor_requires_grad = torch.tensor([1.0], requires_grad=True)\n",
    "print(tensor_requires_grad)\n",
    "# 进行一些操作\n",
    "tensor_result = tensor_requires_grad * 2\n",
    "\n",
    "# 计算梯度,梯度是权重的校准方向，乘学习率得到权重的校准值，学习率指的是每次权重更新以达到最低损失的步长\n",
    "tensor_result.backward()\n",
    "print(tensor_requires_grad.grad)  # 输出梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6094, -0.0760],\n",
      "        [-1.2771, -0.3602]], requires_grad=True)\n",
      "tensor(10.2915, grad_fn=<MeanBackward0>)\n",
      "tensor([[3.9141, 2.8859],\n",
      "        [1.0844, 2.4597]])\n"
     ]
    }
   ],
   "source": [
    "# 创建一个需要计算梯度的张量\n",
    "import torch\n",
    "x = torch.randn(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "# 执行某些操作\n",
    "y = x + 2\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(out)\n",
    "# 反向传播，计算梯度\n",
    "out.backward()\n",
    "\n",
    "# 查看 x 的梯度\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#停止梯度计算\n",
    "#如果你不希望某些张量的梯度被计算（例如，当你不需要反向传播时），可以使用 torch.no_grad() 或设置 requires_grad=False。\n",
    "# 使用 torch.no_grad() 禁用梯度计算\n",
    "with torch.no_grad():\n",
    "    y = x * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNN(\n",
      "  (fc1): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (fc2): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n",
      "tensor([[0.6293]], grad_fn=<AddmmBackward0>)\n",
      "tensor(6.1912, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#神经网络通过调整神经元之间的连接权重来优化预测结果，这一过程涉及前向传播、损失计算、反向传播和参数更新。\n",
    "# PyTorch 提供了 nn 模块，可以方便地构建神经网络。\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 定义一个简单的全连接神经网络\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 2)  # 输入层到隐藏层\n",
    "        self.fc2 = nn.Linear(2, 1)  # 隐藏层到输出层\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))  # ReLU 激活函数\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 创建网络实例\n",
    "model = SimpleNN()\n",
    "\n",
    "# 打印模型结构\n",
    "print(model)\n",
    "\n",
    "# 随机输入\n",
    "x = torch.randn(1, 2)\n",
    "\n",
    "# 前向传播\n",
    "output = model(x)\n",
    "print(output)\n",
    "\n",
    "# 定义损失函数（例如均方误差 MSE）\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 假设目标值为 1\n",
    "target = torch.randn(1, 1)\n",
    "\n",
    "# 计算损失\n",
    "loss = criterion(output, target)\n",
    "print(loss)\n",
    "\n",
    "'''\n",
    "优化器在训练过程中更新神经网络的参数，以减少损失函数的值。\n",
    "\n",
    "PyTorch 提供了多种优化器，例如 SGD、Adam 等。\n",
    "\n",
    "使用优化器进行参数更新：\n",
    "'''\n",
    "# 定义优化器（使用 Adam 优化器）\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练步骤\n",
    "optimizer.zero_grad()  # 清空梯度\n",
    "loss.backward()  # 反向传播\n",
    "optimizer.step()  # 更新参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m x = torch.randn(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# 前向传播\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m output = \u001b[43mmodel\u001b[49m(x)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(output)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# 定义损失函数（例如均方误差 MSE）\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
